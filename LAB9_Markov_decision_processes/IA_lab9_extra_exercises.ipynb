{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jJodIib1_E6tPBvi3BqoxczebfRDQSIC","timestamp":1733494630342}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## EXERCISE 1: What to do at the airport?\n","\n","You are travelling and have some time to kill at the aiport. There are three things you could spend your time doing:\n","  \n","1) You could have a coffee.\n","\n","This has a probability of $0.8$ of giving you time to relax with a tasty beverage, and a utility of $10$.\n","It also has a probability of $0.2$ of providing you with a nasty cup from over-roasted beans that annoys you,\n","and outcome with a utility of $-5$.\n","\n","2) You could shop for clothes.\n","\n","This has a probability of $0.1$ that you will find a great outfit at a good price, utility $20$. However, it\n","has a probability of $0.9$ that you end up wasting money on over-priced junk, utility $-10$.\n","\n","3) You could have a bite to eat.\n","\n","This has a probability of $0.8$ that you find something rather mediocre that prevents you from being too hungry\n","during your flight, utility $2$, and a probability of $0.2$ that you find something filling and tasty, utility $5$.\n","\n","> __QUESTION 1(a):__ What should you do if you take the principle of maximum expected utility to be your decision criterion?\n","\n","> __QUESTION 1(b):__ What should you do if you take the principle of maximax decision criterion to be your decision criterion?\n","\n","> __QUESTION 1(c):__ What should you do if you take the principle of maximin decision criterion to be your decision criterion?\n","    "],"metadata":{"id":"twbLWgpO7YQH"}},{"cell_type":"markdown","source":["> __QUESTION 1(a):__ What should you do if you take the principle of maximum expected utility to be your decision criterion?"],"metadata":{"id":"8nOgGfplV896"}},{"cell_type":"markdown","source":["Coffee"],"metadata":{"id":"z7Sp4_SPYesS"}},{"cell_type":"code","source":["import numpy as np\n","\n","coffe_outputs = np.array([\"tasty\", \"nasty\"])\n","U_coffee_outcomes = np.array([10, -5])\n","P_coffee_outcomes_coffee = np.array([0.8, 0.2])\n","\n","#expected utility\n","eu_coffee_outcomes = U_coffee_outcomes * P_coffee_outcomes_coffee\n","eu_coffee = np.sum(eu_coffee_outcomes)\n","print(eu_coffee)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZzm5uB7W5yI","executionInfo":{"status":"ok","timestamp":1733495451227,"user_tz":-60,"elapsed":353,"user":{"displayName":"Lorenzo Karol Gobbo","userId":"04364812784064389737"}},"outputId":"ce4cd7d4-3021-45be-9f30-fb369bad101a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["7.0\n"]}]},{"cell_type":"markdown","source":["Shopping"],"metadata":{"id":"-YjawHu8Yf9U"}},{"cell_type":"code","source":["shopping_outputs = np.array([\"good_fit\", \"bad_fit\"])\n","U_shopping_outcomes = np.array([20, -10])\n","P_shopping_outcomes_shopping = np.array([0.1, 0.9])\n","\n","#expected utility\n","eu_shopping_outcomes = U_shopping_outcomes * P_shopping_outcomes_shopping\n","eu_shopping = np.sum(eu_shopping_outcomes)\n","print(eu_shopping)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OL5Y1RNDYhZ6","executionInfo":{"status":"ok","timestamp":1733495603332,"user_tz":-60,"elapsed":253,"user":{"displayName":"Lorenzo Karol Gobbo","userId":"04364812784064389737"}},"outputId":"371b89d8-b762-4244-983b-7a95e891dc48"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["-7.0\n"]}]},{"cell_type":"markdown","source":["Eat"],"metadata":{"id":"_HEPY9-aZECH"}},{"cell_type":"code","source":["eat_outputs = np.array([\"good_food\", \"bad_fit\"])\n","U_eat_outcomes = np.array([5, 2])\n","P_eat_outcomes_eat = np.array([0.2, 0.8])\n","\n","#expected utility\n","eu_eat_outcomes = U_eat_outcomes * P_eat_outcomes_eat\n","eu_eat = np.sum(eu_eat_outcomes)\n","print(eu_eat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_mdnm_XZFGm","executionInfo":{"status":"ok","timestamp":1733495760102,"user_tz":-60,"elapsed":225,"user":{"displayName":"Lorenzo Karol Gobbo","userId":"04364812784064389737"}},"outputId":"51b13de3-4b58-4fac-c20e-b1eab246481e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2.6\n"]}]},{"cell_type":"markdown","source":["Following the $MEU$ principle, the best chioce is to go for a coffee."],"metadata":{"id":"LHhQFgNrZlyG"}},{"cell_type":"markdown","source":["> __QUESTION 1(b):__ What should you do if you take the principle of maximax decision criterion to be your decision criterion?\n","\n","The best choice is to go shopping because in the maximax principle you pick the maximum utility of each possible action's outcomes anch pick that action"],"metadata":{"id":"PK8I7buJZ1FS"}},{"cell_type":"markdown","source":["> __QUESTION 1(c):__ What should you do if you take the principle of maximin decision criterion to be your decision criterion?\n","\n","The best choice following the maxmin criterion is to eat something because we pick the worst outcome of all actions and choose the maximum between them."],"metadata":{"id":"9PEKpy3YasZL"}},{"cell_type":"markdown","source":["## EXERCISE 2: Solving a MDP with MDP toolbox\n","\n","We have four states and four actions.\n","\n","The actions are: 0 is Right, 1 is Left, 2 is Up and 3 is Down.\n","\n","The states are 0, 1, 2, 3, and they are arranged like this:\n","    \n","$$\n","\\begin{array}{cc}\n","2 & 3\\\\\n","0 & 1\\\\\n","\\end{array}\n","$$\n","\n","The motion model provides:\n","*   0.8 probability of moving in the direction of the action,\n","*   0.1 probability of moving in each of the directions perpendicular to that of the action.\n","\n","So that 2 is Up from 0 and 1 is Right of 0, and so on. The cost of any action (in any state) is -0.04.\n","\n","In case of \"infeasible\" movements, the agent remains in the current state with probability 0.8+0.1. Consider the perpendicular directions in any case.\n","\n","The reward for state 3 is 1, and the reward for state 1 is -1, and the agent does not leave those states.\n","\n","Set discount factor equal to 0.99.\n","\n","Initial state is 0\n","\n","> __QUESTION 2(a):__ What is the policy based on the Value iteration algorithm?\n","\n","> __QUESTION 2(b):__ What is the policy based on the Policy iteration algorithm?\n","\n","> __QUESTION 2(c):__ What is the policy based on the Q-Learning algorithm?\n","\n","> __QUESTION 2(d):__ Look at the **setVerbose**() function and the time attribute of the MDP objects in MDPToolbox and use them to compare the number of iterations (hint: see the iter attribute) and the CPU time used to come up with a solution (hint: see the time attribute) in the Value iteration algorithm and Policy iteration algorithm resolutions.\n"],"metadata":{"id":"wRLlnsWJ90DZ"}},{"cell_type":"code","source":["!pip install pymdptoolbox\n","import mdptoolbox\n","import numpy as np"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OjHvhyjHbYQc","executionInfo":{"status":"ok","timestamp":1733503791596,"user_tz":-60,"elapsed":3367,"user":{"displayName":"Lorenzo Karol Gobbo","userId":"04364812784064389737"}},"outputId":"1c737a48-1fbd-4f29-93cb-68c22119ec98"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pymdptoolbox in /usr/local/lib/python3.10/dist-packages (4.0b3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.13.1)\n"]}]},{"cell_type":"markdown","source":["Prepare the matrices"],"metadata":{"id":"oWYa-54hfcfV"}},{"cell_type":"code","source":["#probability array\n","#                0   1   2   3         states reached\n","\n","P = np.array([[[0, 0.8, 0.2, 0],  #current state: 0\n","                [0, 1, 0, 0],  #s1\n","                [0.1, 0, 0.1, 0.8],  #s2            #action: Right (0)\n","                [0, 0, 0, 1]], #s3 etc ...\n","               [[0.9, 0, 0.1, 0],\n","                [0, 1, 0, 0],\n","                [0.1, 0, 0.9, 0],      #Left (1)\n","                [0, 0, 0, 1]],\n","               [[0.1, 0.1, 0.8, 0],\n","                [0, 1, 0, 0],\n","                [0, 0, 0.9, 0.1],      #Up (2)\n","                [0, 0, 0, 1]],\n","               [[0.9, 0.1, 0, 0],\n","                [0, 1, 0, 0],\n","                [0.8, 0, 0, 0.2],      #Down (3)\n","                [0, 0, 0, 1]]])\n","\n","\n","#  actions      R    L      U      D\n","\n","R = np.array([[-1, -0.04, -0.04, -0.04],     #s0\n","              [-1, -1, -1, -1],        #s1\n","              [1, -0.04, -0.04, -0.04],      #s2\n","              [1, 1, 1, 1]])            #s3\n","\n","\n","\n","print('Probabilities\\n\\n', P, '\\n\\n\\nRewards\\n\\n', R)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4BiJU6ffeZY","executionInfo":{"status":"ok","timestamp":1733506752377,"user_tz":-60,"elapsed":345,"user":{"displayName":"Lorenzo Karol Gobbo","userId":"04364812784064389737"}},"outputId":"3f8eb9bf-4d98-40a8-8121-da33228f3b51"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Probabilities\n","\n"," [[[0.  0.8 0.2 0. ]\n","  [0.  1.  0.  0. ]\n","  [0.1 0.  0.1 0.8]\n","  [0.  0.  0.  1. ]]\n","\n"," [[0.9 0.  0.1 0. ]\n","  [0.  1.  0.  0. ]\n","  [0.1 0.  0.9 0. ]\n","  [0.  0.  0.  1. ]]\n","\n"," [[0.1 0.1 0.8 0. ]\n","  [0.  1.  0.  0. ]\n","  [0.  0.  0.9 0.1]\n","  [0.  0.  0.  1. ]]\n","\n"," [[0.9 0.1 0.  0. ]\n","  [0.  1.  0.  0. ]\n","  [0.8 0.  0.  0.2]\n","  [0.  0.  0.  1. ]]] \n","\n","\n","Rewards\n","\n"," [[-1.   -0.04 -0.04 -0.04]\n"," [-1.   -1.   -1.   -1.  ]\n"," [ 1.   -0.04 -0.04 -0.04]\n"," [ 1.    1.    1.    1.  ]]\n"]}]},{"cell_type":"code","source":["mdptoolbox.util.check(P, R)"],"metadata":{"id":"lzOVuL0J7LNL","executionInfo":{"status":"ok","timestamp":1733506755866,"user_tz":-60,"elapsed":380,"user":{"displayName":"Lorenzo Karol Gobbo","userId":"04364812784064389737"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["> __QUESTION 2(a):__ What is the policy based on the Value iteration algorithm?\n"],"metadata":{"id":"gZp-x_kkc5xO"}},{"cell_type":"code","source":["vi = mdptoolbox.mdp.ValueIteration(P, R, 0.99)    #gamma = 0.99\n","vi.run()\n","\n","print('Values:\\n', vi.V)\n","print('Policy:\\n', vi.policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yQULGTB_7Qt1","executionInfo":{"status":"ok","timestamp":1733506757035,"user_tz":-60,"elapsed":3,"user":{"displayName":"Lorenzo Karol Gobbo","userId":"04364812784064389737"}},"outputId":"a82e83c8-d9e7-4171-f9c6-c87948dfc54e"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Values:\n"," (89.39593961732311, -99.9949804281095, 98.83037993946596, 99.9949804281095)\n","Policy:\n"," (1, 0, 0, 0)\n"]}]},{"cell_type":"markdown","source":["So, the best actions are: left, right, right, right"],"metadata":{"id":"k6GYEVEl7qrz"}},{"cell_type":"markdown","source":["> __QUESTION 2(b):__ What is the policy based on the Policy iteration algorithm?\n"],"metadata":{"id":"RkSd7B4DE-RC"}},{"cell_type":"code","source":["pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.99)\n","pi.run()\n","print('Values:\\n', pi.V)\n","print('Policy:\\n', pi.policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqPWpyhsE_LD","executionInfo":{"status":"ok","timestamp":1733507183969,"user_tz":-60,"elapsed":298,"user":{"displayName":"Lorenzo Karol Gobbo","userId":"04364812784064389737"}},"outputId":"b69cfa60-3c96-497a-a419-dbe853ac0b3b"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Values:\n"," (89.4009591892136, -99.99999999999991, 98.83539951135636, 99.99999999999991)\n","Policy:\n"," (1, 0, 0, 0)\n"]}]},{"cell_type":"markdown","source":["> __QUESTION 2(c):__ What is the policy based on the Q-Learning algorithm?\n"],"metadata":{"id":"R5P6bB08FfKl"}},{"cell_type":"code","source":["ql = mdptoolbox.mdp.QLearning(P, R, 0.9)\n","ql.run()\n","print('Values:\\n', ql.V)\n","print('Policy:\\n', ql.policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"368XnLrUFaEq","executionInfo":{"status":"ok","timestamp":1733507316010,"user_tz":-60,"elapsed":265,"user":{"displayName":"Lorenzo Karol Gobbo","userId":"04364812784064389737"}},"outputId":"e7baf563-054c-4b69-c789-298a25854284"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Values:\n"," (0.79201207247038, -7.528637632723187, 5.014961486183891, 9.999932089838087)\n","Policy:\n"," (1, 0, 0, 0)\n"]}]}]}